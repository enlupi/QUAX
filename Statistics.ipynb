{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc38390",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d90ad269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson as pois\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nbimporter\n",
    "import prepData as prep\n",
    "import fitFunc as fits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5d2aa",
   "metadata": {},
   "source": [
    "## Significance Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852cb6a",
   "metadata": {},
   "source": [
    "We want to compute the significance of the observed dataset.  \n",
    "The first steps are rescaling the dataset and fitting the background and signal functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6e851d",
   "metadata": {},
   "source": [
    "We then compute the likelihood ratio of the observed data $q_0^{obs}$ between the signal and null hypothesis:\n",
    "\n",
    "$$q_0^{obs} = -2 \\cdot \\log \\left( \\cfrac{\\mathcal{L} \\left(Data | 0, \\hat{\\theta}_0 \\right)}{\\mathcal{L} \\left(Data | \\hat{\\mu}, \\hat{\\theta}_\\hat{\\mu} \\right)}\\right)$$\n",
    "\n",
    "The ^ symbol indicates the values that optimize the fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88568a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute likelihood ratios of two hypotheses\n",
    "def lh_ratio(y, null_vals, alt_vals, N=1365500):\n",
    "    # compute log likelihoods\n",
    "    LogLike_null = sum(norm.logpdf(x=y, loc=null_vals, scale=prep.calc_weights(null_vals, N)))\n",
    "    LogLike_alt  = sum(norm.logpdf(x=y, loc=alt_vals,  scale=prep.calc_weights(alt_vals,  N)))\n",
    "    \n",
    "    #LogLike_null = sum(pois.logpmf(fft.astype(int), model_null.best_fit.astype(int)))\n",
    "    #LogLike_alt  = sum(pois.logpmf(fft.astype(int), model_alt.best_fit.astype(int)))\n",
    "    \n",
    "    # ratio\n",
    "    q = -2 * (LogLike_null - LogLike_alt)\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f59fa",
   "metadata": {},
   "source": [
    "The value of $q_0^{obs}$ has no meaning by itself, so we generate n = 10,000 toy datasets from the expected values given by the background fit and repeate the analysis for every new dataset. Both for the likelihood ratio and for the toy dataset generation, a normal approximation has been used instead of the formal Poisson distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed872673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_toydataset(values, n, N=1365500):\n",
    "    toy_dataset = norm.rvs(loc=values, scale=prep.calc_weights(values, N), size=(n, len(values)))\n",
    "    \n",
    "    #toy_dataset = pois.rvs(mu=values, size=(n,len(values)))\n",
    "    \n",
    "    return toy_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9f79d2",
   "metadata": {},
   "source": [
    "The original $q_0^{obs}$ is thus compared with the distribution of $q_0$ obtained from the toy datasets, and the p-value is computed:\n",
    "\n",
    "$$p_0 = P \\left( q_0 \\ge q_0^{obs} \\right) = \\int_{q_0^{obs}}^{+\\infty} f(q_0 | 0, \\hat{\\theta}_0) \\,dx $$ \n",
    "\n",
    "The significance is expressed as the number of $\\sigma$s needed to achieve an equivalent p-value in a standard normal deviation:\n",
    "\n",
    "$$z = \\Phi^{-1} \\left(1 - p_0 \\right)$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09111118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_value(q_obs, q):\n",
    "    p0 = sum(q >= q_obs)/len(q)\n",
    "    return p0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a74623",
   "metadata": {},
   "source": [
    "This process is repeted using every possible frequency as $x_0$, the center of the signal function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b8a1fd",
   "metadata": {},
   "source": [
    "## Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5cf72",
   "metadata": {},
   "source": [
    "The process is similar to that of the significance test, but with a few key differences.  \n",
    "\n",
    "In addition to fitting the background, we fit the signal twice: one time we let the $\\mu$ run free to find $\\hat{\\mu}$, while the other we keep it fixed to a certain value $\\mu$.  \n",
    "The likelihood ratio is thus computed as:\n",
    "\n",
    "$$q^{obs} \\left( \\mu \\right) = -2 \\cdot \\log \\left( \\cfrac{\\mathcal{L} \\left(Data | \\mu, \\hat{\\theta}_{\\mu} \\right)}{\\mathcal{L} \\left(Data | \\hat{\\mu}, \\hat{\\theta}_\\hat{\\mu} \\right)}\\right)$$\n",
    "\n",
    "We then generate two sets of n = 10,000 toy datasets each, one as before from the expected values given by the background fit while the other from the signal fit with fixed $\\mu$. We compare the original $q^{obs}(\\mu)$ with the distribution of $q(0)$ and $q(\\mu)$ from the toy datasets and compute the two probabilities:\n",
    "\n",
    "$$    p_{\\mu} = P \\left(q(\\mu) \\ge q^{obs}(\\mu) | \\mu s + b \\right)$$\n",
    "$$1 - p_{b}   = P \\left(q(\\mu) \\ge q^{obs}(\\mu) | b \\right)$$\n",
    "\n",
    "and take their ratio.  \n",
    "This process is done scanning different values of $\\mu$ and we take as the 95% confidence interval limit the value of $\\mu$ so that the ratio is equal to 0.05:\n",
    "\n",
    "$$\\mu^{95 \\%CL} = \\mu \\; \\big| \\;CL_S = \\cfrac{p_{\\mu}}{1 - p_{b}} = 0.05$$\n",
    "\n",
    "This process is repeted using every possible frequency as $x_0$, the center of the signal function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ebf12c",
   "metadata": {},
   "source": [
    "## Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b49becde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_test(run, x_0=np.array([]), mu_fix=60, signal=fits.signal_gauss, n_toy=10000,\n",
    "              calc_z=False, calc_CI=False, draw=False, verbose=False, path='db/'):\n",
    "    \n",
    "    # load and prep data\n",
    "    data, center, length = prep.load_dataset(run, path)\n",
    "    freq, fft, weights, ref, N = prep.prep_data(data, center, length=length)\n",
    "    \n",
    "    # fit background once\n",
    "    res_bkg = fits.fit_bkg(x=freq, y=fft, w=weights, center=center, ref=ref)\n",
    "    bkg        = res_bkg.best_fit\n",
    "    bkg_params = res_bkg.best_values\n",
    "    \n",
    "    # generate toy datasets from bkg and fit them\n",
    "    toy_0 = gen_toydataset(values=bkg, n=n_toy, N=N)\n",
    "        \n",
    "        \n",
    "    # scan x0 and perform desired tests\n",
    "    z     = np.empty(len(x_0))\n",
    "    mu_CI = np.empty(len(x_0))\n",
    "    \n",
    "    for i_x0 in range(len(x_0)):\n",
    "        \n",
    "        # fit signal\n",
    "        sig = fits.fit_sig(x=freq, y=fft, w=weights, x_0=x_0[i_x0], init_params=bkg_params, signal=signal).best_fit\n",
    "        \n",
    "        # compute significance\n",
    "        if calc_z:\n",
    "            z[i_x0] = significance(freq, fft, bkg, sig, toy_0, center, ref, x_0[i_x0], signal, N, draw)\n",
    "            \n",
    "        # compute 95% CI mu\n",
    "        if calc_CI:\n",
    "            mu_CI[i_x0] = CI(freq, fft, weights,\n",
    "                             bkg_params, center, ref,\n",
    "                             x_0[i_x0], signal, mu_fix, sig,\n",
    "                             toy_0, N, draw, verbose)\n",
    "            mu_fix = mu_CI[i_x0]\n",
    "            \n",
    "        if (i_x0+1)%20 == 0:\n",
    "            print(\"Step:\", i_x0+1)\n",
    "            \n",
    "    return z, mu_CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95b5df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def significance(x, y, bkg, sig, toy_0, center, ref, x_0, signal, N=1365500, draw=False):\n",
    "    \n",
    "    # compute likelihood ratio of observed data\n",
    "    q0_obs = lh_ratio(y, bkg, sig, N)     \n",
    "    \n",
    "    n_toy = len(toy_0)\n",
    "    \n",
    "    # analyze toy datasets\n",
    "    q0 = np.empty(n_toy)\n",
    "    for i_toy in range(n_toy):\n",
    "        # fit background over toy dataset\n",
    "        toy_w = prep.calc_weights(toy_0[i_toy], N)\n",
    "        res_toy_bkg = fits.fit_bkg(x=x, y=toy_0[i_toy], w=toy_w, center=center, ref=ref)\n",
    "        toy_bkg = res_toy_bkg.best_fit\n",
    "        toy_bkg_params = res_toy_bkg.best_values\n",
    "        # fit signal\n",
    "        toy_sig = fits.fit_sig(x=x, y=toy_0[i_toy], w=toy_w, x_0=x_0,\n",
    "                               init_params=toy_bkg_params, signal=signal).best_fit\n",
    "      \n",
    "        # compue likelihood ratio of toy dataset \n",
    "        q0[i_toy] = lh_ratio(toy_0[i_toy], toy_bkg, toy_sig, N)\n",
    "    \n",
    "    # plot significance distribution\n",
    "    if(draw):\n",
    "        plot_lhratio(q0_obs, q0, x_0=x_0)\n",
    "        \n",
    "    # compute significance\n",
    "    p0 = p_value(q0_obs, q0)\n",
    "    z = norm.ppf(1-p0)\n",
    "        \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4fed739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CI(x, y, w, bkg_params, center, ref, x_0, signal, mu_fix, sig, toy_0, N=1365500, draw=False, verbose=False):\n",
    "    \n",
    "    n_toy = len(toy_0)\n",
    "    \n",
    "    # scan for mu\n",
    "    q_mu_obs_prev = 0            # save distribution and parameters\n",
    "    q_mu_prev = np.empty(n_toy)  # to plot optimal result\n",
    "    q0_prev = np.empty(n_toy)\n",
    "    r_prev = 1e10\n",
    "    sign_prev = 0\n",
    "    mu_95 = 0\n",
    "    \n",
    "    while True: \n",
    "        \n",
    "        fix = fits.fit_sig(x, y, w, x_0, bkg_params, signal, mu_init=mu_fix, mu_vary=False).best_fit\n",
    "        \n",
    "        # compute likelihood ratio of observed data\n",
    "        q_mu_obs = lh_ratio(y, fix, sig)\n",
    "        \n",
    "        # generate toy datasets from fixed mu\n",
    "        toy_fix = gen_toydataset(fix, n_toy, N)\n",
    "        \n",
    "        # comute distriution of likelihood ratios\n",
    "        q_mu = calc_qmu(x, toy_fix, center, ref, x_0, mu_fix, signal, N)\n",
    "        q0   = calc_qmu(x, toy_0,   center, ref, x_0, mu_fix, signal, N)\n",
    "        \n",
    "        # compute p-values\n",
    "        p_mu = p_value(q_mu_obs, q_mu)\n",
    "        p_b  = p_value(q_mu_obs, q0)\n",
    "        \n",
    "        # compute ratio\n",
    "        r = p_mu/p_b\n",
    "        \n",
    "        if(verbose):\n",
    "            print(\"Mu: \", mu_fix, \"   q(mu)_obs =\", q_mu_obs,\n",
    "                  \"\\np_mu =\", p_mu, \"  p_b =\", p_b, \"  ratio =\", r, \"\\n\")\n",
    "            \n",
    "        # check results to proceed with the mu scan:\n",
    "        # if the ratio is close eneough to target we save the results and stop\n",
    "        # else we check if we are under- or overshooting and correct the estimate\n",
    "        # if we cross the target we stop and take the best result between\n",
    "        # current and previous step\n",
    "        reached_target, crossed_target, is_current_worse = False, False, False\n",
    "        reached_target = (np.abs(r - 0.05) <= 0.01)\n",
    "        if not reached_target:\n",
    "            sign = np.sign(r - 0.05)\n",
    "            crossed_target = (sign*sign_prev == -1)\n",
    "            if crossed_target:\n",
    "                is_current_worse = (np.abs(r - 0.05) > np.abs(r_prev - 0.05))\n",
    "        \n",
    "        # update best estimate for every case except the last\n",
    "        if not is_current_worse:\n",
    "            q_mu_obs_prev = q_mu_obs            \n",
    "            q_mu_prev = q_mu  \n",
    "            q0_prev = q0\n",
    "            mu_95 = mu_fix\n",
    "            \n",
    "        if reached_target or crossed_target:\n",
    "            break\n",
    "        \n",
    "        # update mu if we did not exit the loop\n",
    "        mu_fix = mu_fix + sign*5\n",
    "        r_prev = r\n",
    "        sign_prev = sign\n",
    "            \n",
    "    # plot significance distribution\n",
    "    if(draw):\n",
    "        plot_lhratio(q_mu_obs_prev, q0_prev, q_mu_prev, x_0, mu_95)\n",
    "        \n",
    "    return(mu_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cad3223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_qmu(x, toy, center, ref, x_0, mu_fix, signal, N=1365500):\n",
    "    \n",
    "    n_toy = len(toy)\n",
    "    \n",
    "    # compute likelihood ratio for toy dataset\n",
    "    q_mu = np.empty(n_toy)\n",
    "    for i_toy in range(n_toy):\n",
    "        # compute signal and fixed mu signal over toy dataset\n",
    "        toy_w = prep.calc_weights(toy[i_toy], N)\n",
    "        toy_bkg_params = fits.fit_bkg(x, toy[i_toy], toy_w, center, ref).best_values\n",
    "        toy_fix = fits.fit_sig(x, toy[i_toy], toy_w, x_0, toy_bkg_params, signal,\n",
    "                               mu_init=mu_fix, mu_vary=False).best_fit\n",
    "        toy_sig = fits.fit_sig(x, toy[i_toy], toy_w, x_0, toy_bkg_params, signal).best_fit\n",
    "            \n",
    "        # likelihood ratio\n",
    "        q_mu[i_toy] = lh_ratio(toy[i_toy], toy_fix, toy_sig, N)\n",
    "        \n",
    "    return q_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a01f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lhratio(q_obs, q0, q_alt=np.array([]), x_0=0, mu_95=0):\n",
    "    # prepare canvas\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    \n",
    "    # plot q0 distribution\n",
    "    #N = len(q0)\n",
    "    #binning = int(np.sqrt(N))\n",
    "    n, bins, _ = plt.hist(q0, bins=\"auto\", density=True, alpha=0.5,\n",
    "                          facecolor='lightblue', edgecolor='black', label='Toy Experiments: background')\n",
    "    lineheight = max(n)\n",
    "    if np.any(q_alt):\n",
    "        n_alt, bins_alt, _ = plt.hist(q_alt, bins=\"auto\", density=True, alpha=0.5,\n",
    "                                      facecolor='salmon', edgecolor='black', label='Toy Experiments: signal')\n",
    "        lineheight = max(lineheight, max(n_alt))\n",
    "    \n",
    "    plt.vlines(q_obs, 0, lineheight, colors='forestgreen', linestyles='--', label='Observed Data')\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    title = \"X_0 = \" + str(x_0)\n",
    "    if np.any(q_alt):\n",
    "        title = title + \"    mu 95%CL = \" + str(mu_95)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('q')\n",
    "    plt.ylabel('PDF')\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
