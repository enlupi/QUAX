{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc38390",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d90ad269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson as pois\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nbimporter\n",
    "import prepData as prep\n",
    "import fitFunc as fits\n",
    "\n",
    "import multiprocess as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5d2aa",
   "metadata": {},
   "source": [
    "## Significance\n",
    "\n",
    "Assuming the presence of a signal, we want to compute its significance on the observed dataset.  \n",
    "\n",
    "The first steps are rescaling the dataset and fitting the background and signal functions.  \n",
    "We then compute the likelihood ratio of the observed data $q_0^{obs}$ between the signal and null hypothesis:\n",
    "\n",
    "$$q_0^{obs} = -2 \\cdot \\log \\left( \\cfrac{\\mathcal{L} \\left(Data | 0, \\hat{\\theta}_0 \\right)}{\\mathcal{L} \\left(Data | \\hat{\\mu}, \\hat{\\theta}_\\hat{\\mu} \\right)}\\right)$$\n",
    "\n",
    "The ^ symbol indicates the values that optimize the fits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f59fa",
   "metadata": {},
   "source": [
    "The value of $q_0^{obs}$ has no meaning by itself, so we generate n = 10,000 toy datasets from the expected values given by the background fit and repeat the analysis for every new dataset. Both for the likelihood ratio and for the toy dataset generation, a normal approximation has been used instead of the formal Poisson distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9f79d2",
   "metadata": {},
   "source": [
    "The original $q_0^{obs}$ is thus compared with the distribution of $q_0$ obtained from the toy datasets, and the p-value is computed:\n",
    "\n",
    "$$p_0 = P \\left( q_0 \\ge q_0^{obs} \\right) = \\int_{q_0^{obs}}^{+\\infty} f(q_0 | 0, \\hat{\\theta}_0) \\,dx $$ \n",
    "\n",
    "The significance is expressed as the number of $\\sigma$s needed to achieve an equivalent p-value in a standard normal distribution:\n",
    "\n",
    "$$z = \\Phi^{-1} \\left(1 - p_0 \\right)$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a74623",
   "metadata": {},
   "source": [
    "This process is repeted using every possible frequency as $x_0$, the center of the signal function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5cf72",
   "metadata": {},
   "source": [
    "## Confidence Interval\n",
    "\n",
    "The goal of this analysis is to set a limit on the strength of the signal. In order to do so, we scan over certain values of $\\mu$ and do the following analysis.\n",
    "\n",
    "In addition to fitting the background, we fit the signal twice: one time we let the $\\mu$ run free to find $\\hat{\\mu}$, while the other we keep it fixed to a certain value $\\mu$.  \n",
    "The likelihood ratio is then computed as:\n",
    "\n",
    "$$q^{obs} \\left( \\mu \\right) = -2 \\cdot \\log \\left( \\cfrac{\\mathcal{L} \\left(Data | \\mu, \\hat{\\theta}_{\\mu} \\right)}{\\mathcal{L} \\left(Data | \\hat{\\mu}, \\hat{\\theta}_\\hat{\\mu} \\right)}\\right)$$\n",
    "\n",
    "We then generate two sets of n = 10,000 toy datasets each, one as before from the expected values given by the background fit while the other from the signal fit with fixed $\\mu$. We compare the original $q^{obs}(\\mu)$ with the distribution of $q(\\mu)$ from the toy datasets and compute the two probabilities:\n",
    "\n",
    "$$    p_{\\mu} = P \\left(q(\\mu) \\ge q^{obs}(\\mu) | \\mu s + b \\right)$$\n",
    "$$1 - p_{b}   = P \\left(q(\\mu) \\ge q^{obs}(\\mu) | b \\right)$$\n",
    "\n",
    "and take their ratio.  \n",
    "\n",
    "$$CL_S = \\cfrac{p_{\\mu}}{1 - p_{b}}$$\n",
    "\n",
    "Finally, we take as the 95% confidence interval limit the value of $\\mu$ so that the ratio is equal to 0.05:\n",
    "\n",
    "$$\\mu^{95 \\%CL} = \\mu \\; \\big| \\;CL_S = 0.05$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ebf12c",
   "metadata": {},
   "source": [
    "## Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582db24c",
   "metadata": {},
   "source": [
    "### Toy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed872673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_toydataset(values, n, N=1365500):\n",
    "    toy_dataset = norm.rvs(loc=values, scale=prep.calc_weights(values, N), size=(n, len(values)))\n",
    "\n",
    "    return toy_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286dfa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_toydataset(InfoDataset, n, data_type=\"fft\"):\n",
    "    res_list = []\n",
    "    for i in range(n):\n",
    "        run_list = []\n",
    "        for i_run, run in enumerate(InfoDataset):\n",
    "            toy_data = gen_toydataset(run[data_type], 1, run[\"N\"]).flatten()\n",
    "            toy_dict = {\"name\":run[\"name\"],\n",
    "                        \"N\":run[\"N\"],\n",
    "                        \"center\":run[\"center\"],\n",
    "                        \"ref\":run[\"ref\"],\n",
    "                        \"freq\":run[\"freq\"],\n",
    "                        \"fft\":toy_data,\n",
    "                        \"weights\":prep.calc_weights(toy_data, run[\"N\"])}\n",
    "            run_list.append(toy_dict)\n",
    "        res_list.append(run_list)\n",
    "        \n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c45488e",
   "metadata": {},
   "source": [
    "### Likelihood ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88568a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute likelihood ratios of two hypotheses\n",
    "def lh_ratio(y, null_vals, alt_vals, null_weights=None, alt_weights=None, N=1365500):\n",
    "    \n",
    "    if null_weights is None:\n",
    "        null_weights = prep.calc_weights(null_vals, N)\n",
    "    if alt_weights is None:\n",
    "        alt_weights = prep.calc_weights(alt_vals, N)\n",
    "    \n",
    "    # compute log likelihoods\n",
    "    LogLike_null = sum(norm.logpdf(x=y, loc=null_vals, scale=null_weights))\n",
    "    LogLike_alt  = sum(norm.logpdf(x=y, loc=alt_vals,  scale=alt_weights))\n",
    "    \n",
    "    #LogLike_null = sum(pois.logpmf(fft.astype(int), model_null.best_fit.astype(int)))\n",
    "    #LogLike_alt  = sum(pois.logpmf(fft.astype(int), model_alt.best_fit.astype(int)))\n",
    "    \n",
    "    # ratio\n",
    "    q = -2 * (LogLike_null - LogLike_alt)\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e28acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_lh_ratio(data_list, name1, null_list, name2, alt_list, name3):\n",
    "    # get dataset to compute likelihood ratio on from each list of results\n",
    "    y = np.zeros(shape=(len(data_list), len(data_list[0][name1])))\n",
    "    null         = np.zeros_like(y)\n",
    "    null_weights = np.zeros_like(y)\n",
    "    alt          = np.zeros_like(y)\n",
    "    alt_weights  = np.zeros_like(y)\n",
    "    for i_run, run in enumerate(data_list):\n",
    "        y[i_run] = run[name1]\n",
    "        null[i_run] = null_list[i_run][name2]\n",
    "        null_weights[i_run] = null_list[i_run][\"weights\"]\n",
    "        alt[i_run] = alt_list[i_run][name3]\n",
    "        alt_weights[i_run] = alt_list[i_run][\"weights\"]\n",
    "        \n",
    "    q = lh_ratio(y.flatten(), null.flatten(), alt.flatten(),\n",
    "                 null_weights.flatten(), alt_weights.flatten())\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "777e8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_calc_qmu(toyData, x0,mu_fix):\n",
    "    \n",
    "    n_toy = len(toyData)\n",
    "    \n",
    "    # compute likelihood ratio for toy dataset\n",
    "    q_mu = np.empty(n_toy)\n",
    "    for i_toy,toy in enumerate(toyData):\n",
    "        \n",
    "        toy_bkg_params = fits.multipleFitBKG(toy)\n",
    "        \n",
    "        fix_toy=fits.multipleFitSIG(toy, toy_bkg_params, x_0=x0,mu_init=mu_fix,mu_vary=False)\n",
    "        fitSig_toy=fits.multipleFitSIG(toy, toy_bkg_params, x_0=x0)\n",
    "        \n",
    "        q_mu[i_toy] = multiple_lh_ratio(toy, \"fft\",fix_toy, \"sig_bestFit\",fitSig_toy,\"sig_bestFit\")\n",
    "        \n",
    "    return q_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09111118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_value(q_obs, q):\n",
    "    p0 = sum(q >= q_obs)/len(q)\n",
    "    return p0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96636024",
   "metadata": {},
   "source": [
    "### Complete test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f07d61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_test(InfoDataset,x_0,mu_fix=1.0,nToy=10000,calc_CI=False,calc_z=False,verbose=False,draw=False):\n",
    "    \n",
    "    #fit background once\n",
    "    fitBkg = fits.multipleFitBKG(InfoDataset)\n",
    "    \n",
    "    # generate toy datasets from bkg and fit them\n",
    "    toy_0 = multiple_toydataset(fitBkg, n=nToy, data_type=\"bkg_bestFit\")\n",
    "    \n",
    "    z     = np.empty(len(x_0))\n",
    "    mu_CI = np.empty(len(x_0))\n",
    "    r = np.empty(len(x_0))\n",
    "    \n",
    "    for i_x0,x0 in enumerate(x_0):\n",
    "        \n",
    "        fitSig = fits.multipleFitSIG(InfoDataset, fitBkg, x_0=x0)\n",
    "        \n",
    "        # compute significance\n",
    "        if calc_z:\n",
    "            z[i_x0] = multipleSignificance(InfoDataset, fitBkg, fitSig, toy_0, x0, draw)\n",
    "        \n",
    "        if calc_CI:\n",
    "            if i_x0 == 0:\n",
    "                mu_CI[i_x0] = multipleCI(InfoDataset,fitBkg,fitSig,toy_0,x0,mu_fix,verbose,draw)\n",
    "        \n",
    "            else:\n",
    "                mu_CI[i_x0] = multipleCI(InfoDataset,fitBkg,fitSig,toy_0,x0,mu_CI[i_x0-1],verbose,draw)\n",
    "        \n",
    "            if verbose:\n",
    "                print(\"Testing x0:\",x0)\n",
    "                print(\"mu_CI:\",mu_CI[i_x0],\"\\tr:\",r[i_x0])\n",
    "                print(\"---------------------\")\n",
    "    \n",
    "    return z,mu_CI    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95b5df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multipleSignificance(InfoDataset, fitBkg, fitSig, toy_0, x_0, draw=False):\n",
    "    \n",
    "    # compute likelihood ratio of observed data\n",
    "    q0_obs = multiple_lh_ratio(InfoDataset, \"fft\",\n",
    "                               fitBkg, \"bkg_bestFit\",\n",
    "                               fitSig, \"sig_bestFit\")     \n",
    "    \n",
    "    n_toy = len(toy_0)\n",
    "    \n",
    "    # analyze toy datasets\n",
    "    q0 = np.empty(n_toy)\n",
    "    for i_toy in range(n_toy):\n",
    "        # fit background over toy dataset\n",
    "        toy_fitBkg = fits.multipleFitBKG(toy_0[i_toy])\n",
    "        # fit signal\n",
    "        toy_fitSig = fits.multipleFitSIG(toy_0[i_toy], toy_fitBkg, x_0)\n",
    "      \n",
    "        # compue likelihood ratio of toy dataset \n",
    "        q0[i_toy] =  multiple_lh_ratio(toy_0[i_toy], \"fft\",\n",
    "                                       toy_fitBkg, \"bkg_bestFit\",\n",
    "                                       toy_fitSig, \"sig_bestFit\")\n",
    "    \n",
    "    # plot significance distribution\n",
    "    if(draw):\n",
    "        plot_lhratio(q0_obs, q0, x_0=x_0)\n",
    "        \n",
    "    # compute significance\n",
    "    p0 = p_value(q0_obs, q0)\n",
    "    z = norm.ppf(1-p0)\n",
    "        \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39e3ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multipleCI(InfoDataset, fitBkg, fitSig,toy_0,x_0, mu_fix,verbose=False,draw=False):\n",
    "    \n",
    "    n_toy = len(toy_0)\n",
    "    \n",
    "    # scan for mu\n",
    "    q_mu_obs_prev = 0            # save distribution and parameters\n",
    "    q_mu_prev = np.empty(n_toy)  # to plot optimal result\n",
    "    q0_prev = np.empty(n_toy)\n",
    "    \n",
    "    r_prev = 1e10\n",
    "    mu_95 = 0\n",
    "    \n",
    "    sign_prev = 0\n",
    "        \n",
    "    mu_test=mu_fix\n",
    "    cross_check=0\n",
    "    \n",
    "    all_mu=[]\n",
    "    while True:\n",
    "                \n",
    "        if mu_test in all_mu:\n",
    "            break\n",
    "        \n",
    "        fix = fits.multipleFitSIG(InfoDataset, fitBkg, x_0=x_0,mu_init=mu_test,mu_vary=False)\n",
    "        \n",
    "        # compute likelihood ratio of observed data\n",
    "        q_mu_obs = multiple_lh_ratio(InfoDataset, \"fft\",\n",
    "                                       fix, \"sig_bestFit\",\n",
    "                                       fitSig, \"sig_bestFit\")\n",
    "        \n",
    "        if(verbose):\n",
    "            print(\"Mu: \", mu_test, \"   q(mu)_obs =\", q_mu_obs)\n",
    "        \n",
    "        # generate toy datasets from fixed mu\n",
    "        toy_fix = multiple_toydataset(fix, n=n_toy, data_type=\"sig_bestFit\")\n",
    "        \n",
    "        q_mu=multiple_calc_qmu(toy_fix,x_0,mu_test)\n",
    "        q0=multiple_calc_qmu(toy_0,x_0,mu_test)\n",
    "        \n",
    "        # compute p-values\n",
    "        p_mu = p_value(q_mu_obs, q_mu)\n",
    "        p_b  = p_value(q_mu_obs, q0)\n",
    "        \n",
    "        # compute ratio\n",
    "        r = p_mu/p_b\n",
    "            \n",
    "        if(verbose):\n",
    "            print(\"p_mu =\", p_mu, \"  p_b =\", p_b, \"  ratio =\", r)\n",
    "            print(cross_check)\n",
    "            print(all_mu, '\\n')\n",
    "            \n",
    "        # check results to proceed with the mu scan:\n",
    "        # if the ratio is close eneough to target we save the results and stop\n",
    "        # else we check if we are under- or overshooting and correct the estimate\n",
    "        # if we cross the target we stop and take the best result between\n",
    "        # current and previous step\n",
    "        \n",
    "                \n",
    "        #check if the next mu has been already tested\n",
    "        all_mu.append(mu_test)\n",
    "        \n",
    "        if math.isnan(r) or math.isinf(r):\n",
    "            mu_test = mu_test - 3\n",
    "            if mu_test < 0: mu_test=1\n",
    "            continue\n",
    "            \n",
    "        reached_target, crossed_target, is_current_worse = False, False, False\n",
    "        \n",
    "        reached_target = (np.abs(r - 0.05) <= 0.01)\n",
    "        if not reached_target:\n",
    "            sign = np.sign(r - 0.05)\n",
    "            crossed_target = (sign*sign_prev == -1)\n",
    "            \n",
    "            if crossed_target:\n",
    "                is_current_worse = (np.abs(r - 0.05) > np.abs(r_prev - 0.05))\n",
    "                cross_check+=1\n",
    "        \n",
    "        # update best estimate for every case except the last\n",
    "        if not is_current_worse:\n",
    "            q_mu_obs_prev = q_mu_obs            \n",
    "            q_mu_prev = q_mu  \n",
    "            q0_prev = q0\n",
    "            mu_95 = mu_test\n",
    "            \n",
    "        if reached_target or cross_check==2:\n",
    "            cross_check=0\n",
    "            break\n",
    "        \n",
    "        #adaptive step algorithm\n",
    "        if r==0: r=0.14\n",
    "        \n",
    "        step=0   \n",
    "        check=r-0.05\n",
    "        \n",
    "        if np.abs(check) > 0.5:\n",
    "            step = sign*2\n",
    "        elif ((np.abs(check) > 0.2) and (np.abs(check) < 0.5)):\n",
    "            step = sign*7/5\n",
    "        elif ((np.abs(check) > 0.08) and (np.abs(check) < 0.2)):\n",
    "            step = sign\n",
    "        else:\n",
    "            step = sign*2/5\n",
    "            \n",
    "        # update mu if we did not exit the loop\n",
    "        mu_test = mu_test + step*5\n",
    "        r_prev = r\n",
    "        sign_prev = sign\n",
    "\n",
    "            \n",
    "        if mu_test <= 0:\n",
    "            mu_test=1\n",
    "            \n",
    "    # plot significance distribution\n",
    "    if(draw):\n",
    "        plot_lhratio(q_mu_obs_prev, q0_prev, q_mu_prev, x_0, mu_95)\n",
    "            \n",
    "    return(mu_95, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a0aa0a",
   "metadata": {},
   "source": [
    "### Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cd7ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_test_parallel(runsData, x0, mu_init, njobs=10):\n",
    "    \n",
    "    x_0 = np.split(x0, njobs)\n",
    "    manager = mp.Manager()\n",
    "    results = manager.list()\n",
    "    \n",
    "    def worker(runsData, x_0, mu_init, results):\n",
    "        result = stat_test(runsData, x_0, mu_init)\n",
    "        results.append(result)\n",
    "    \n",
    "    processes = []\n",
    "    for i in range(njobs):\n",
    "        p = mp.Process(target=worker, args=(runsData, x_0[i], mu_init[i], results))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "    \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "        \n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad7d69",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a01f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lhratio(q_obs, q0, q_alt=np.array([]), x_0=0, mu_95=0):\n",
    "    # prepare canvas\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    \n",
    "    # plot q0 distribution\n",
    "    #N = len(q0)\n",
    "    #binning = int(np.sqrt(N))\n",
    "    n, bins, _ = plt.hist(q0, bins=\"auto\", density=True, alpha=0.5,\n",
    "                          facecolor='lightblue', edgecolor='black', label='Toy Experiments: background')\n",
    "    lineheight = max(n)\n",
    "    if np.any(q_alt):\n",
    "        n_alt, bins_alt, _ = plt.hist(q_alt, bins=\"auto\", density=True, alpha=0.5,\n",
    "                                      facecolor='salmon', edgecolor='black', label='Toy Experiments: signal')\n",
    "        lineheight = max(lineheight, max(n_alt))\n",
    "    \n",
    "    plt.vlines(q_obs, 0, lineheight, colors='forestgreen', linestyles='--', label='Observed Data')\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    title = \"X_0 = \" + str(x_0)\n",
    "    if np.any(q_alt):\n",
    "        title = title + \"    mu 95%CL = \" + str(mu_95)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('q')\n",
    "    plt.ylabel('PDF')\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
